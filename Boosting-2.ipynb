{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847adbb7-db76-4977-981e-bb4bb638709c",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd065e-f3c9-4607-a19d-30f13d51dd2b",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "Gradient Boosting Regression is a machine learning algorithm that belongs to the family of boosting techniques. It is used for regression tasks, where the goal is to predict continuous numeric values. Gradient Boosting Regression builds an ensemble of weak learners (typically decision trees) sequentially. Each weak learner is trained to correct the errors made by the previous ones, focusing on the residual errors of the previous model. The final prediction is the weighted sum of predictions from all weak learners, where the weights are determined based on their performance during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768ab2e-88b4-43b3-a1dd-6c72edbb4923",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491180f2-5135-481c-9aa5-c7f7f6e92a5f",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c8edb9-4e1e-477c-a364-b94ab71036a4",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "Here's a simple implementation of Gradient Boosting Regression from scratch using Python and NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d9e3048-53af-4340-b1a7-0b97fcee40e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 18.318697169379433\n",
      "R-squared: 0.9880701046491472\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a synthetic dataset for regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=5, random_state=42)\n",
    "\n",
    "# Define the Gradient Boosting Regression class\n",
    "class GradientBoostingRegression:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the prediction with the mean of the target values\n",
    "        pred = np.mean(y) * np.ones_like(y)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the negative gradient (residuals)\n",
    "            residuals = y - pred\n",
    "\n",
    "            # Fit a weak learner (decision tree) to the negative gradient\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the prediction with the weighted prediction of the tree\n",
    "            pred += self.learning_rate * tree.predict(X)\n",
    "\n",
    "            # Store the model and its weight\n",
    "            self.models.append(tree)\n",
    "            self.weights.append(self.learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by combining predictions from all weak learners\n",
    "        pred = np.zeros(len(X))\n",
    "        for i in range(self.n_estimators):\n",
    "            pred += self.weights[i] * self.models[i].predict(X)\n",
    "        return pred\n",
    "\n",
    "# Train the Gradient Boosting Regression model\n",
    "gb_model = GradientBoostingRegression(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_model.fit(X, y)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_pred = gb_model.predict(X)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7fe27-1fe2-442b-9990-ddf2b4958328",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acec684-dfcf-48b9-919d-91939ae4fe94",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d565c-385c-439b-9285-61ed94caf9fa",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "To experiment with different hyperparameters, we can use grid search or random search techniques. Here's an example using grid search:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37fb4461-93ed-41e3-a922-b1283c3f86b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best Mean Squared Error: 35.40829719488946\n",
      "Best R-squared: 0.9769406483342594\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Generate a synthetic dataset for regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=5, random_state=42)\n",
    "\n",
    "# Define the Gradient Boosting Regression class\n",
    "class GradientBoostingRegression(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the prediction with the mean of the target values\n",
    "        pred = np.mean(y) * np.ones_like(y)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the negative gradient (residuals)\n",
    "            residuals = y - pred\n",
    "\n",
    "            # Fit a weak learner (decision tree) to the negative gradient\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the prediction with the weighted prediction of the tree\n",
    "            pred += self.learning_rate * tree.predict(X)\n",
    "\n",
    "            # Store the model and its weight\n",
    "            self.models.append(tree)\n",
    "            self.weights.append(self.learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by combining predictions from all weak learners\n",
    "        pred = np.zeros(len(X))\n",
    "        for i in range(self.n_estimators):\n",
    "            pred += self.weights[i] * np.array(self.models[i].predict(X))\n",
    "        return pred\n",
    "\n",
    "# Create a scikit-learn compatible wrapper for GradientBoostingRegression\n",
    "from sklearn.base import clone\n",
    "\n",
    "class GradientBoostingRegressorWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.gb_model = GradientBoostingRegression(n_estimators=n_estimators,\n",
    "                                                   learning_rate=learning_rate,\n",
    "                                                   max_depth=max_depth)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.gb_model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.gb_model.predict(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth\n",
    "        }\n",
    "\n",
    "# Now we can use the GradientBoostingRegressorWrapper in grid search or random search\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the GradientBoostingRegressorWrapper\n",
    "gb_model = GradientBoostingRegressorWrapper()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model's performance\n",
    "y_pred_best = best_model.predict(X)\n",
    "mse_best = mean_squared_error(y, y_pred_best)\n",
    "r2_best = r2_score(y, y_pred_best)\n",
    "\n",
    "print(\"Best Mean Squared Error:\", mse_best)\n",
    "print(\"Best R-squared:\", r2_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68aa9b-06e7-48ba-9454-ab662e3c7fc2",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d8454-0f91-4c58-9047-9330eaa2c9b0",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc33b3-63e6-4865-85f7-d4d8babbb1e0",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "In Gradient Boosting, a weak learner refers to a simple and relatively low-complexity model that performs slightly better than random guessing on a specific task. In most cases, decision trees are used as weak learners. These decision trees are shallow (limited depth) to avoid overfitting and typically have just a few splits. Weak learners are combined in an ensemble to create a strong learner that can make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbaf43-6ff9-49ad-b189-2a56c6f188e8",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83dbde1-120b-428e-9820-73b17987c381",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c6469c-811c-4261-854e-d172b6171204",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "The intuition behind Gradient Boosting is to iteratively improve the model's performance by sequentially adding weak learners to the ensemble. In each iteration, the algorithm identifies the errors made by the current ensemble and focuses on these errors in the next iteration. By giving more weight to the misclassified instances, each subsequent weak learner corrects the mistakes made by its predecessors. This iterative process continues until a predefined number of weak learners is reached or until the model's performance converges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd56842-96dd-463e-9a53-abe5f7acc2c6",
   "metadata": {},
   "source": [
    "                       -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfcd8a-e250-42a7-8424-c3a5f8fe7a69",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ec4f9-dded-413f-b97e-d4ac1759a65a",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. The process can be summarized as follows:\n",
    "\n",
    ">Initialize the prediction with the mean of the target values (or any other suitable initial value).\n",
    "\n",
    ">Compute the negative gradient (residuals) of the current prediction with respect to the true target values.\n",
    "\n",
    ">Fit a weak learner (e.g., decision tree) to the negative gradient.\n",
    "\n",
    ">Update the prediction by adding the weighted prediction of the weak learner to the current prediction.\n",
    "\n",
    ">Repeat steps 2 to 4 for a predefined number of iterations (n_estimators) or until the performance converges.\n",
    "\n",
    ">The final prediction is the weighted sum of predictions from all weak learners, where the weights are determined based on their performance during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195c9bd-aba6-49ad-a195-6b541eb54fb2",
   "metadata": {},
   "source": [
    "                        -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f5010-8426-42d8-88ce-318675d1395c",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455a521-5fe1-4deb-921a-b6842cf84a55",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "The mathematical intuition behind the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    ">Define a loss function: The loss function measures the difference between the predicted values and the true target values. It serves as a guide for the algorithm to minimize errors during training.\n",
    "\n",
    ">Initialize the prediction: Start with an initial prediction, often set to the mean of the target values.\n",
    "\n",
    ">Compute the negative gradient (residuals): Calculate the negative gradient (or pseudo-residuals) of the loss function with respect to the current prediction. These residuals represent the errors made by the current ensemble.\n",
    "\n",
    ">Fit a weak learner to the negative gradient: Train a weak learner (e.g., decision tree) on the features and the negative gradient. The weak learner tries to predict the negative gradient to correct the errors made by the current model.\n",
    "\n",
    ">Update the prediction: Update the prediction by adding the weighted prediction of the weak learner to the current prediction. The weight is determined by a learning rate, which controls the contribution of each weak learner.\n",
    "\n",
    ">Repeat steps 3 to 5: Iterate the process by computing the negative gradient of the new prediction and fitting another weak learner to correct the new errors.\n",
    "\n",
    ">Combine the weak learners: The final prediction is the weighted sum of predictions from all weak learners, where the weights are determined based on their performance during training.\n",
    "\n",
    ">These steps are repeated for a predefined number of iterations (n_estimators) to build an ensemble of weak learners that create a strong predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3b616-2f74-4a73-88b6-1f04edafdd88",
   "metadata": {},
   "source": [
    "                        -------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
